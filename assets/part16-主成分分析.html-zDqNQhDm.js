import{_ as l}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as t,c as e,b as h,a as s,g as i,i as a,o as p}from"./app-DfRKKd8F.js";const k={},r=s("h2",{id:"数据维度和降维",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#数据维度和降维"},[s("span",null,"数据维度和降维")])],-1),d=s("p",null,[i("数据维度指的是数据集中的特征数量或变量数量。每个特征都可以看作一个维度，因此如果一个数据集有 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"n")]),s("annotation",{encoding:"application/x-tex"},"n")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"n")])])]),i(" 个特征，那么这个数据集可以被认为是处于 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"n")]),s("annotation",{encoding:"application/x-tex"},"n")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"n")])])]),i(" 维空间中。例如，在一个包含三个特征（如高度、体重和年龄）的数据集里，每个数据点可以表示为三维空间中的一个点。")],-1),c=a('<p><strong>降维</strong>（Dimensionality Reduction）是将高维数据映射到低维空间的过程，同时尽可能保留原始数据的重要信息。降维的主要目的是：</p><ol><li><p><strong>减少计算复杂度</strong>：高维数据在处理时往往需要更多的计算资源，通过降维可以减少计算时间和存储空间。</p></li><li><p><strong>消除噪声</strong>：高维数据中可能包含一些对最终分析没有贡献的特征，通过降维可以去除这些无关或噪声特征。</p></li><li><p><strong>提高模型性能</strong>：某些情况下，模型在低维数据上表现得更好，因为降维后数据的结构可能更加简单明了。</p></li><li><p><strong>可视化</strong>：降维能将数据映射到2D或3D空间，便于数据的可视化和直观理解。</p></li></ol><p>常见的降维方法有：</p><ul><li><strong>主成分分析 (PCA)</strong>：通过将数据投影到主成分方向上，保留最大方差的维度。</li><li><strong>线性判别分析 (LDA)</strong>：通过最大化类间方差与类内方差之比来寻找最优投影方向。</li><li><strong>t-SNE</strong>：一种非线性降维技术，常用于将高维数据降到2D或3D，以便于可视化。</li><li><strong>UMAP</strong>：一种快速且强大的非线性降维方法，特别适合大型数据集的可视化。</li></ul><p>降维在数据分析、机器学习、图像处理等多个领域都有广泛应用。</p><h2 id="主成分分析" tabindex="-1"><a class="header-anchor" href="#主成分分析"><span>主成分分析</span></a></h2><p>主成分分析（Principal Component Analysis，PCA）是一种广泛使用的降维技术，用于从高维数据中提取重要信息，同时减少数据的维度。PCA 通过线性变换将原始特征转换为一组新的不相关特征（称为主成分），这些主成分能够解释数据中尽可能多的方差。</p><h3 id="pca-的基本步骤" tabindex="-1"><a class="header-anchor" href="#pca-的基本步骤"><span>PCA 的基本步骤</span></a></h3>',8),E=s("ol",null,[s("li",null,[s("p",null,[s("strong",null,"标准化数据"),i("：")]),s("ul",null,[s("li",null,"因为不同特征的量纲可能不同，因此需要对每个特征进行标准化处理（如减去均值后除以标准差），使得所有特征具有相同的量级。")])]),s("li",null,[s("p",null,[s("strong",null,"计算协方差矩阵"),i("：")]),s("ul",null,[s("li",null,[i("协方差矩阵描述了数据集不同特征之间的关系。它的计算公式为："),s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",{mathvariant:"normal"},"Σ"),s("mo",null,"="),s("mfrac",null,[s("mn",null,"1"),s("mrow",null,[s("mi",null,"n"),s("mo",null,"−"),s("mn",null,"1")])]),s("msup",null,[s("mi",null,"X"),s("mi",null,"T")]),s("mi",null,"X")]),s("annotation",{encoding:"application/x-tex"}," \\Sigma = \\frac{1}{n-1} X^T X ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord"},"Σ"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"2.0908em","vertical-align":"-0.7693em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.3214em"}},[s("span",{style:{top:"-2.314em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"n"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"−"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mord"},"1")])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.677em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7693em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})]),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8913em"}},[s("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.13889em"}},"T")])])])])])])]),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X")])])])])]),i(" 其中 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"X")]),s("annotation",{encoding:"application/x-tex"},"X")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07847em"}},"X")])])]),i(" 是标准化后的数据矩阵，"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"n")]),s("annotation",{encoding:"application/x-tex"},"n")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"n")])])]),i(" 是样本数量。")])])]),s("li",null,[s("p",null,[s("strong",null,"计算协方差矩阵的特征值和特征向量"),i("：")]),s("ul",null,[s("li",null,"特征值和特征向量分别表示数据的方差及其在特征空间中的方向。特征值越大，表示对应的特征向量方向上数据的方差越大。")])]),s("li",null,[s("p",null,[s("strong",null,"选择主成分"),i("：")]),s("ul",null,[s("li",null,"根据特征值的大小，对应的特征向量被称为主成分。通常选择前几个特征值最大的主成分作为新特征空间的基底。选择多少主成分取决于你希望保留的数据方差比例。")])]),s("li",null,[s("p",null,[s("strong",null,"转换数据"),i("：")]),s("ul",null,[s("li",null,"最后，将原始数据投影到选择的主成分上，从而获得降维后的数据。")])])],-1),g=a('<h3 id="pca-的特点和应用" tabindex="-1"><a class="header-anchor" href="#pca-的特点和应用"><span>PCA 的特点和应用</span></a></h3><ul><li><p><strong>无监督学习</strong>：PCA 是一种无监督学习算法，因为它不依赖于任何标签信息，仅基于数据的特征结构进行降维。</p></li><li><p><strong>解释性</strong>：PCA 使得我们能够理解数据的结构，例如哪个主成分在多大程度上解释了数据的变化。</p></li><li><p><strong>可视化</strong>：PCA 常用于高维数据的可视化，通过将数据映射到2D或3D空间，使数据结构更加直观。</p></li></ul><h3 id="示例" tabindex="-1"><a class="header-anchor" href="#示例"><span>示例</span></a></h3>',3),o=s("p",null,[i("假设你有一个数据集，包含 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"m")]),s("annotation",{encoding:"application/x-tex"},"m")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"m")])])]),i(" 个样本和 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"n")]),s("annotation",{encoding:"application/x-tex"},"n")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"n")])])]),i(" 个特征。应用 PCA 后，你可能发现前两个主成分解释了数据总方差的80%，因此你可以选择这两个主成分来减少维度，并将数据从 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"n")]),s("annotation",{encoding:"application/x-tex"},"n")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"n")])])]),i(" 维空间投影到2D空间进行可视化。")],-1),m=a(`<h3 id="使用-python-进行-pca" tabindex="-1"><a class="header-anchor" href="#使用-python-进行-pca"><span>使用 Python 进行 PCA</span></a></h3><p>在 Python 中，你可以使用 <code>scikit-learn</code> 库中的 <code>PCA</code> 类来执行主成分分析。以下是一个简单的示例代码：</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;"><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.decomposition </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> PCA</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.preprocessing </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> StandardScaler</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> numpy </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 假设 X 是你的数据矩阵，大小为 (m, n)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.array([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">...</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 1. 数据标准化</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">scaler </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> StandardScaler()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X_scaled </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> scaler.fit_transform(X)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 2. 创建 PCA 对象并拟合数据</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">pca </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> PCA(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">n_components</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 保留前两个主成分</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X_pca </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pca.fit_transform(X_scaled)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 3. 查看主成分解释的方差比例</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">explained_variance </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pca.explained_variance_ratio_</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Explained variance ratio: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">explained_variance</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="优缺点" tabindex="-1"><a class="header-anchor" href="#优缺点"><span>优缺点</span></a></h3><p><strong>优点</strong>：</p><ul><li>能够有效地降低数据维度，减少计算复杂度。</li><li>通过最大化方差保留数据的最重要信息。</li><li>可以消除特征之间的相关性，生成的主成分是互不相关的。</li></ul><p><strong>缺点</strong>：</p><ul><li>PCA 是一种线性方法，对非线性关系无法很好地处理。</li><li>主成分可能难以解释，因为它们是原始特征的线性组合。</li><li>数据的方差可能集中在一个维度中，从而可能会忽略其他潜在重要的信息。</li></ul><p>PCA 是一种强大的工具，适用于各种数据分析场景，尤其是在处理高维数据时。</p><h2 id="python实现主成分数据降维" tabindex="-1"><a class="header-anchor" href="#python实现主成分数据降维"><span>python实现主成分数据降维</span></a></h2><p>为了实现PCA数据降维，并动态模拟降维过程，你可以使用Python中的<code>scikit-learn</code>和<code>matplotlib</code>库。以下是一个完整的示例代码，它包括了PCA降维的实现以及动态模拟降维过程的可视化。</p><h3 id="_1-导入必要的库" tabindex="-1"><a class="header-anchor" href="#_1-导入必要的库"><span>1. 导入必要的库</span></a></h3><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;"><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> numpy </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> matplotlib.pyplot </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> plt</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.decomposition </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> PCA</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sklearn.datasets </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> make_blobs</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> mpl_toolkits.mplot3d </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Axes3D</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> matplotlib.animation </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> animation</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-生成示例数据" tabindex="-1"><a class="header-anchor" href="#_2-生成示例数据"><span>2. 生成示例数据</span></a></h3><p>这里我们生成一个三维的样本数据集，这样可以更直观地看到降维过程。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;"><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 生成样本数据</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X, _ </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> make_blobs(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">n_samples</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">n_features</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">centers</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">random_state</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">42</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 绘制原始3D数据</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">fig </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> plt.figure()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ax </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> fig.add_subplot(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">111</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">projection</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;3d&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ax.scatter(X[:, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], X[:, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], X[:, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ax.set_title(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Original 3D Data&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.show()</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_3-实现pca并逐步降维" tabindex="-1"><a class="header-anchor" href="#_3-实现pca并逐步降维"><span>3. 实现PCA并逐步降维</span></a></h3><p>在这个步骤中，我们将逐步计算主成分并可视化数据在降维过程中的变化。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;"><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 标准化数据</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X_mean </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.mean(X, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">axis</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X_centered </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> X </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> X_mean</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 计算协方差矩阵</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cov_matrix </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.cov(X_centered.T)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 计算协方差矩阵的特征值和特征向量</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">eigenvalues, eigenvectors </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.linalg.eig(cov_matrix)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 对特征值排序并选择前两个主成分</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sorted_indices </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.argsort(eigenvalues)[::</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sorted_eigenvectors </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> eigenvectors[:, sorted_indices]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">pca_components </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sorted_eigenvectors[:, :</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 动态模拟降维过程</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">fig, ax </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> plt.subplots()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ax.set_xlim(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ax.set_ylim(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sc </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ax.scatter(X[:, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], X[:, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> update</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(frame):</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 在第 frame 步时，计算投影到第 frame 个主成分的结果</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    projected_data </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> X_centered </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sorted_eigenvectors[:, :frame</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    sc.set_offsets(projected_data)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    ax.set_title(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Dimension: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">frame</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sc,</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ani </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> animation.FuncAnimation(fig, update, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">frames</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">repeat</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.show()</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_4-可视化完整的降维过程" tabindex="-1"><a class="header-anchor" href="#_4-可视化完整的降维过程"><span>4. 可视化完整的降维过程</span></a></h3><p>最后，将数据投影到前两个主成分上，并可视化最终结果。</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;"><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 投影到前两个主成分</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X_pca </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> X_centered </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pca_components</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 可视化最终降维结果</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.scatter(X_pca[:, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], X_pca[:, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.title(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;2D Projection using PCA&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.xlabel(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Principal Component 1&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.ylabel(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;Principal Component 2&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.grid(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">plt.show()</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="解释代码" tabindex="-1"><a class="header-anchor" href="#解释代码"><span>解释代码</span></a></h3><ul><li><strong>make_blobs</strong>：用于生成具有簇状结构的3D数据集。</li><li><strong>cov_matrix</strong>：计算样本数据的协方差矩阵，反映特征之间的相关性。</li><li><strong>eigenvalues, eigenvectors</strong>：通过特征值分解获得特征值和特征向量，特征向量代表主成分的方向。</li><li><strong>FuncAnimation</strong>：用于动态更新散点图，从而模拟降维的过程。</li></ul><p>通过上面的代码，你可以动态地模拟PCA降维的过程，并看到数据从3D逐步被投影到2D空间中的变化。</p><h2 id="scikit-learn数据降维" tabindex="-1"><a class="header-anchor" href="#scikit-learn数据降维"><span>scikit-learn数据降维</span></a></h2>`,26);function y(u,v){const n=t("PDF");return p(),e("div",null,[r,d,c,E,g,o,m,h(n,{url:"//jianghu105.github.io/assets/pdffile/scikit-learn数据降维.pdf"})])}const A=l(k,[["render",y],["__file","part16-主成分分析.html.vue"]]),D=JSON.parse('{"path":"/documents/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/part16-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90.html","title":"主成分分析","lang":"zh-CN","frontmatter":{"cover":"/assets/images/cover2.jpg","title":"主成分分析","date":"2024-08-16T00:00:00.000Z","icon":"pen-to-square","author":null,"isOriginal":true,"category":["数学建模"],"tag":["数学建模"],"sticky":1,"star":1,"article":true,"timeline":true,"license":null,"description":"数据维度和降维 数据维度指的是数据集中的特征数量或变量数量。每个特征都可以看作一个维度，因此如果一个数据集有 n 个特征，那么这个数据集可以被认为是处于 n 维空间中。例如，在一个包含三个特征（如高度、体重和年龄）的数据集里，每个数据点可以表示为三维空间中的一个点。 降维（Dimensionality Reduction）是将高维数据映射到低维空间的过...","head":[["meta",{"property":"og:url","content":"https://jianghu105.github.io/documents/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/part16-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90.html"}],["meta",{"property":"og:site_name","content":"Jianghu"}],["meta",{"property":"og:title","content":"主成分分析"}],["meta",{"property":"og:description","content":"数据维度和降维 数据维度指的是数据集中的特征数量或变量数量。每个特征都可以看作一个维度，因此如果一个数据集有 n 个特征，那么这个数据集可以被认为是处于 n 维空间中。例如，在一个包含三个特征（如高度、体重和年龄）的数据集里，每个数据点可以表示为三维空间中的一个点。 降维（Dimensionality Reduction）是将高维数据映射到低维空间的过..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://jianghu105.github.io/assets/images/cover2.jpg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://jianghu105.github.io/assets/images/cover2.jpg"}],["meta",{"name":"twitter:image:alt","content":"主成分分析"}],["meta",{"property":"article:author","content":"Jianghu"}],["meta",{"property":"article:tag","content":"数学建模"}],["meta",{"property":"article:published_time","content":"2024-08-16T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"主成分分析\\",\\"image\\":[\\"https://jianghu105.github.io/assets/images/cover2.jpg\\"],\\"datePublished\\":\\"2024-08-16T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Jianghu\\",\\"url\\":\\"https://jianghu105.github.io/intro.html\\"}]}"]]},"headers":[{"level":2,"title":"数据维度和降维","slug":"数据维度和降维","link":"#数据维度和降维","children":[]},{"level":2,"title":"主成分分析","slug":"主成分分析","link":"#主成分分析","children":[{"level":3,"title":"PCA 的基本步骤","slug":"pca-的基本步骤","link":"#pca-的基本步骤","children":[]},{"level":3,"title":"PCA 的特点和应用","slug":"pca-的特点和应用","link":"#pca-的特点和应用","children":[]},{"level":3,"title":"示例","slug":"示例","link":"#示例","children":[]},{"level":3,"title":"使用 Python 进行 PCA","slug":"使用-python-进行-pca","link":"#使用-python-进行-pca","children":[]},{"level":3,"title":"优缺点","slug":"优缺点","link":"#优缺点","children":[]}]},{"level":2,"title":"python实现主成分数据降维","slug":"python实现主成分数据降维","link":"#python实现主成分数据降维","children":[{"level":3,"title":"1. 导入必要的库","slug":"_1-导入必要的库","link":"#_1-导入必要的库","children":[]},{"level":3,"title":"2. 生成示例数据","slug":"_2-生成示例数据","link":"#_2-生成示例数据","children":[]},{"level":3,"title":"3. 实现PCA并逐步降维","slug":"_3-实现pca并逐步降维","link":"#_3-实现pca并逐步降维","children":[]},{"level":3,"title":"4. 可视化完整的降维过程","slug":"_4-可视化完整的降维过程","link":"#_4-可视化完整的降维过程","children":[]},{"level":3,"title":"解释代码","slug":"解释代码","link":"#解释代码","children":[]}]},{"level":2,"title":"scikit-learn数据降维","slug":"scikit-learn数据降维","link":"#scikit-learn数据降维","children":[]}],"git":{},"readingTime":{"minutes":6.92,"words":2077},"filePathRelative":"documents/数学建模/part16-主成分分析.md","localizedDate":"2024年8月16日","excerpt":"","autoDesc":true}');export{A as comp,D as data};
